<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
<title type="text">Rodion's blog</title>
<subtitle type="text"></subtitle>
<generator uri="https://github.com/mojombo/jekyll">Jekyll</generator>
<link rel="self" type="application/atom+xml" href="/feed.xml" />
<link rel="alternate" type="text/html" href="" />
<updated>2015-03-22T20:48:40+03:00</updated>
<id>/</id>
<author>
  <name>Rodion</name>
  <uri>/</uri>
  <email>rodion.zheludkov@gmail.com</email>
</author>


<entry>
  <title type="html"><![CDATA[CAP теорема]]></title>
  <link rel="alternate" type="text/html" href="/cap" />
  <id>/cap</id>
  <published>2015-03-22T00:00:00+03:00</published>
  <updated>2015-03-22T00:00:00+03:00</updated>
  <author>
    <name>Rodion</name>
    <uri></uri>
    <email>rodion.zheludkov@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;CAP теорема обычно всплывает в обсуждении распределенных систем и звучит она так “Between consistency (C), availability (A) and partition tolerance (P) you can pick any two. It is impossible to achieve all three”. Обычно в такой формулировке не очень понятно, что такое вообще partition tolerance и что за система (CA) получится если мы от него откажемся? Для начала рассмотрим более детальные определения каждой части&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Consistency. Или согласованность. Если данные записаны на кластер, то все следующие чтения эти данные увидят. Самая сильная форма согласованности - данные по всему кластере одинаковые и их можно читать/писать с любой машины и результат будет всегда одинаковый.&lt;/li&gt;
  &lt;li&gt;Availability. Кластер доступен даже если сколько-то машин выпало.&lt;/li&gt;
  &lt;li&gt;Partition-tolerance. Кластер продолжает работать, даже если есть сетевая ошибка, которая разделит машины на две непустые группы, которые не могут общаться между собой.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;С такими определениями становится совсем не понятно, что значит система в которой нет partition tolerance (CA). Есть другое определение P из оригинальной статьи: “In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another”. То есть P это не свойство кластера, а свойство сети, которое нельзя игнорировать. В таком случае CAP теорема должна звучать примерно так: в распределенной системе во время образования network partition можно либо потерять согласованность (потому что разрешаем апдейты на обе части кластера), либо потерять доступность (потому что выключаем кластер пока сетевая проблема не устранена). &lt;/p&gt;

&lt;p&gt;Другими словами CA система - система в которой данные согласованы и кластер доступен пока не произойдет сетевой сбой (P). Если сбой произойдет то данные разъедутся и не восстановятся, когда сетевая проблема будет устранена. Работоспособной такая система может быть, если сетевых проблем не бывает, то есть когда система - большой монолитный объект, который если падает, то падает целиком. Но такая система по определению не является распределенной.&lt;/p&gt;

&lt;h2 id=&quot;cp-and-ap&quot;&gt;CP and AP&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;CP. Данные согласованы по всем нодам. Если происходит P - кластер останавливает все операции, которые работали с данными, задетыми проблемой.&lt;/li&gt;
  &lt;li&gt;AP. При P ноды остаются онлайн и синхронизируют данные позже, когда проблема будет решена. Не гарантируется, что на всех нодах будут те же данные (во время или даже после того как проблема устранена). &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;CP и AP системы асимметричны&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;CP. Доступностью жертвуют только тогда, когда возникает проблема&lt;/li&gt;
  &lt;li&gt;AP. Согласованностью жертвуют всегда. В лучше случае система может гарантировать только “eventual consistency”. В этом случае иногда система может вернуть данные отличные от только что записанных. Плюс клиенты, читающие данные по одному ключу, могут получать разные результаты. Может получится так, что часть апдейтов не доедет до всех реплик или в одну реплику приедут конфликтующие апдейты. Решение таких конфликтов остается на совести системы. Можно пытаться мержить такие апдейты, например, используя vector clock алгоритм, в кассандре это называется read repair. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Надо понимать, что в реальности согласованность и доступность не всегда полные. Например, HDFS, по умолчанию пишет в три ноды, если все они сломаются, то система станет недоступной. Zookeeper отвечает на запись ОК, если больше половины кворума сказали ОК, он станет недоступным если выпадут больше половины машин.&lt;/p&gt;

&lt;h2 id=&quot;latency&quot;&gt;Latency&lt;/h2&gt;

&lt;p&gt;Одна из проблема с CAP подходом состоит в том, что из рассмотрения выкинули задержки. Иногда распределенные системы жертвуют согласованностью не для того, чтобы улучшить доступность, а чтобы снизить время ответа. Например можно отказаться от согласованности в пользу так называемой “timeline consistency”, в этом случае реплики могут быть не согласованы между собой, но есть гарантия того, что апдейты будут применяться в одном порядке ко всем репликам. &lt;/p&gt;

&lt;p&gt;Для гарантии согласованности надо, до того как ответить ОК на запись, убедится, что сколько-то реплик получили и применили это запись. На это нужно время. Если говорить ОК сразу после записи в мастер-реплику, а репликацию делать асинхронно - задержки уменьшатся. Но в этом случае пострадает и доступность, если выпала мастер-реплика для части данных, то эти данные становятся недоступны для обновления. &lt;/p&gt;

&lt;h2 id=&quot;pacelc&quot;&gt;PACELC&lt;/h2&gt;

&lt;p&gt;Умные люди придумали альтернативу CAP, которая учитывает все выше перечисленное и облегчает понимание архитектуры распределенной системы. &lt;/p&gt;

&lt;p&gt;PACELC — if there is a partition (P) how does the system trade off between availability and consistency (A and C); else (E) when the system is running as normal in the absence of partitions, how does the system trade off between latency (L) and consistency (C)?&lt;/p&gt;

&lt;p&gt;Кассандра например PA/EL. Если есть P, то выбираем доступность. Если сетевых проблем нет - жертвуем согласованностью ради скорости.&lt;/p&gt;

&lt;p&gt;ACID базы данных PC/EC. Если реплики не могут общаться - все записи запрещаются. Пока все реплики не скажут ОК, данные не запишутся - жертвуем скоростью.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;p&gt;Пост родился в попытках разобраться с сабжем и по сути является переработкой с переводом и компановкой следующих статей:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://dbmsmusings.blogspot.ru/2010/04/problems-with-cap-and-yahoos-little.html&quot;&gt;http://dbmsmusings.blogspot.ru/2010/04/problems-with-cap-and-yahoos-little.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/&quot;&gt;http://blog.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cacm.acm.org/blogs/blog-cacm/83396-errors-in-database-systems-eventual-consistency-and-the-cap-theorem/fulltext&quot;&gt;http://cacm.acm.org/blogs/blog-cacm/83396-errors-in-database-systems-eventual-consistency-and-the-cap-theorem/fulltext&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html&quot;&gt;http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://stackoverflow.com/questions/12346326/nosql-cap-theorem-availability-and-partition-toleranc&quot;&gt;http://stackoverflow.com/questions/12346326/nosql-cap-theorem-availability-and-partition-toleranc&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

  &lt;p&gt;&lt;a href=&quot;/cap&quot;&gt;CAP теорема&lt;/a&gt; was originally published by Rodion at &lt;a href=&quot;&quot;&gt;Rodion's blog&lt;/a&gt; on March 22, 2015.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Principal Component Analysis]]></title>
  <link rel="alternate" type="text/html" href="/pca" />
  <id>/pca</id>
  <published>2014-08-13T00:00:00+04:00</published>
  <updated>2014-08-13T00:00:00+04:00</updated>
  <author>
    <name>Rodion</name>
    <uri></uri>
    <email>rodion.zheludkov@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;Как выглядит задача машинного обучения в реальной жизни? Сели, подумали, выбрали какие-то фичи, засунули их в алгоритм, получили результат. Чем больше фич выбрали, тем, вроде как, лучше. Пример: пытаемся описать модель плавного подъема воздушного шарика; иногда он подлетает на воздушном потоке, иногда его качает немного в стороны. Для самой простой модели нам всего лишь надо знать среднюю скорость, а для этого нужна одна камера, которая сделает несколько снимков сбоку. По снимкам мы посчитаем среднюю скорость. А теперь предположим: мы застряли в облаке, парим рядом с шариком не знаем, где земля, и поэтому не знаем где находится “сбоку”. Поэтому ставим камеры как-нибудь, побольше и со всех сторон. Вопрос, как теперь из этого вычленить реальное нужное нам измерение &lt;em&gt;y&lt;/em&gt;, чтобы посчитать в итоге скорость?&lt;/p&gt;

&lt;p&gt;Идея PCA как раз в том чтобы перевыразить наши данные через новый базис, который представит их наилучим образом и при этом поможет отфильтровать шум. В нашем примере надо найти комбинацию измерений с камер, наилучшим образом представлющие нашу координату &lt;em&gt;y&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Если выражать математически, есть sample выраженный через $m$ -мерный вектор наблюдений $x$. В нашем пример один sample - это набор фотографий в определенный момент времени, а $x$ - все измерения, снятые с этих фотографий. $X$ - это матрица всех наших наблюдений. &lt;/p&gt;

&lt;p&gt;Теперь надо вспомнить немного линейной алгебры. Каждый вектор можно представить как линейную комбинацию некоторых базисных векторов. В 2мерном пространстве самый простой базис - это (1,0); (0,1). PCA - это способ преобразовать наш простой базис в другой, являющийся линейной комбинацией оригинального. То есть мы линейно преобразуем наши исходные данные:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; PX = Y &lt;/script&gt;

&lt;p&gt;Здесь $P$ - матрица преобразования, которую нам и предстоит найти. Для этого надо формально определить, что значит “представить данные наилучшим образом”. &lt;/p&gt;

&lt;h2 id=&quot;noise&quot;&gt;Шум (Noise)&lt;/h2&gt;

&lt;p&gt;Картинка с одной из камер.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/it/pca.1.png&quot; alt=&quot;Camera 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Видно, что координаты шарика на фотографии складываются в растянутое облако точек. Считаем, что направления, по которым наши измерения меняются больше всего, представляют как раз итересующую нас динамику системы. Иначе это значит, что шум, который мы снимаем, забивает реальные измерения. Идея в том, чтобы повернуть наш базис так, чтобы его вектора расположились по направлениям наибольшей дисперсии.&lt;/p&gt;

&lt;h2 id=&quot;redundancy&quot;&gt;Лишние данные (Redundancy)&lt;/h2&gt;

&lt;p&gt;Ясно, что мы наснимали сильно больше фоток, чем нужно. Более того, скорее всего одни картинки выражаются простым линейным преобразованием из других. Например, поставили одну камеру на метр выше второй. Тогда координаты с этой камеры линейно выражаются ($coord_1 = coord_2 + 1$). И на самом деле мы эти фотографии можем просто выкинуть, они не несут нам никакой информации. Хочется от таких измерений избавится совсем.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;Ковариационная матрица&lt;/h2&gt;

&lt;p&gt;Мера линейной зависимости между переменными - &lt;em&gt;covariance&lt;/em&gt;. Мера разброса значений переменной - &lt;em&gt;variance&lt;/em&gt;. Ковариационная матрица содержит как раз все эти меры и считается как:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; C_X = \frac{1}{n}XX^T &lt;/script&gt;

&lt;p&gt;$X$ - матрица, строки которой представляют собой наблюдения от разных семплов. В нашем случае одной строкой будет значения координаты $x$ с первой камеры в каждый момент времени.&lt;/p&gt;

&lt;p&gt;Теперь, если вернуться к нашим целям и идельному представлению Y, то мы хотим получить ковариационную матрицу $C_Y$, для которой&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Все ковариации равны 0.&lt;/li&gt;
  &lt;li&gt;Все строки отсортированы по значимости согласно значению &lt;em&gt;variance&lt;/em&gt;. Таким образом, мы в любой момент сможем взять, например, лучшие 10 измерений, а остальные выкинуть как ненужные.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Если рассматривать задачу с точки зрения статистики - мы пытаемся преобразовать данные таким образом, при котором сможем вычленить независимые компоненты.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; C_Y = \frac{1}{n}YY^T  = \frac{1}{n}(PX)(PX)^T = \frac{1}{n}P (XX)^T P^T = P C_X P^T &lt;/script&gt;

&lt;p&gt;Любую матрицу можно представить как разложение в собственных векторах и диаганальной матрицы $A = EDE^T$. Тогда, взяв за $P = E^T$, где $E^T$ матрица собственных векторов $C_X$ получим:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; C_Y = P C_X P^T = P (E D E^T) P^T = P (P^T D P) P^T &lt;/script&gt;

&lt;p&gt;Учитывая, что $P$ - ортогональная матрица $P^T = P^{-1}$, получаем, что $C_Y = D$. $P$ - ортогональная матрица, так как $C_X$ - симметричная и вещественная матрица. Для таких матриц всегда существует ортономированный базис из собственных векторов и разложение через ортогональную матрицу $P$.&lt;/p&gt;

&lt;p&gt;То есть мы нашли нужные вектора разложения, которые оказались собственными векторами матрицы $C_X$. Первый собственный вектор - это первый компонент. По этому направлению дисперсия максимальна.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;Допущения&lt;/h2&gt;

&lt;p&gt;Следущие допущения нужны для того, чтобы применить PCA&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Линейность. Если один аттрибут выражается через другой нелинейно, PCA не поможет. Поможет, например, предварительная замена аттрибутов, чтобы избавится от нелинейности.&lt;/li&gt;
  &lt;li&gt;Полагаем, что нас интересуют больше всего направления, в которых &lt;em&gt;variance&lt;/em&gt; максимален. Это не всегда правда.&lt;/li&gt;
&lt;/ol&gt;


  &lt;p&gt;&lt;a href=&quot;/pca&quot;&gt;Principal Component Analysis&lt;/a&gt; was originally published by Rodion at &lt;a href=&quot;&quot;&gt;Rodion's blog&lt;/a&gt; on August 13, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[Онлайн деревья решений]]></title>
  <link rel="alternate" type="text/html" href="/decision-trees" />
  <id>/decision-trees</id>
  <published>2014-06-21T00:00:00+04:00</published>
  <updated>2014-06-21T00:00:00+04:00</updated>
  <author>
    <name>Rodion</name>
    <uri></uri>
    <email>rodion.zheludkov@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;Задачи машинного обучения обычно сводятся к тому, чтобы посторить модель предсказания реальности и минимизировать ее ошибку. В зависимости от выбора модели и функции ошибки мы получим разные алгоритмы. Но можно рассматривать эту задачу немного с другой стороны: ошибки - это разница между нашей моделью и реальным миром. Чем больше модель ошибается, тем больше неожиданных предсказаний мы получим и, сравнивая предсказания с реальностью, мы сильно &lt;em&gt;удивимся&lt;/em&gt;. &lt;/p&gt;

&lt;p&gt;То есть в каком-то смысле задача состоит в том, чтобы уменьшить наше удивление и сделать модель, соответсвующую реальности максимально. Собственно, первый же вопрос, который возникает “а как это удивление мерить?”. Этим занимается отдельная наука - теория информации. Она рассматривает информацию, связанную с исходом случайной величины и отвечает на вопросы вида “сколько информации несет нам знание того, что на кубике выпала “6”. Интуитивно информация соответствует мере удивления при появлении какого-то исхода. Чем больше мы удивлемся, тем больше информации получаем. Выбираются следущие аксиомы:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$I(p)$ непрерывная монотонно уменьшающаяся функция от $p$. $p$ - вероятность рассматриваемого исхода&lt;/li&gt;
  &lt;li&gt;Если A и B независимые события. $P(A) = p_1, P(B) = p_2, P(AB)=p_1p_2$. Тогда $I(p_1p_2) = I(p_1) + I(p_2)$. &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;1-ая аксима по сути означает, что менее вероятному событию мы удивляемся больше. Например, выпадению 6x6 на двух кубиках мы удивимся больше, чем выпадению орла при бросании монеты. &lt;/p&gt;

&lt;p&gt;2-ая аксиома говорит, что удивления от невлияющих друг на друга событий складывается из удивлений от каждого отдельного события. Тоже все логично и не противоречит интуитивному пониманию.&lt;/p&gt;

&lt;p&gt;Можно показать, что единственное семейство функций, которое удовлетворяет этим аксиомам, это &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; I = -\log{p_i} &lt;/script&gt;

&lt;p&gt;Теперь нам нужна некая единица измерения информации. Возьмем за 1 бит неопределенность, связанную с исходом случайной величины, которая может быть в двух состояниях с вероятностью = 1/2. По сути это удивления от исхода после броска монеты. Тогда база логарифма будет 2, а мера изменения будет называться “бит”. Важно, что это не тот же бит что в компьютерах (принимающий только значения 0 и 1). Информация может измеряться и в дробных значениях. Это просто мера удивления, например, 1.5 бита значит, что мы в полтора раза больше удивляемся данному исходу по сравнению с результатом броска монеты.&lt;/p&gt;

&lt;p&gt;Вообще говоря, нас интересует не один конкретный исход, а все множество исходов, образующих случайную величину. Мы хотим уметь сравнивать разные случайные величины по тому, насколько одна содержит больше информации (“удивительнее”) другой. Тут все просто: берем мат. ожидание удивления от всех событий&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; H = -\sum{p_i}*\log{p_i} &lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;$I$ - это мера удивления от одного исхода; &lt;/li&gt;
  &lt;li&gt;$H$ - мера удивления всей случайной величины. Она называется &lt;strong&gt;entropy&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section&quot;&gt;Переходим непосредственно к деревьям решений&lt;/h2&gt;

&lt;p&gt;Теперь нам надо построить дерево решений. Первым шагом надо выбрать узел, который максимально отделит одни классы от других. Если рассматривать эту задачу с точки зрения теории информации, нам надо выбрать узел таким образом, чтобы наше удивление уменьшилось максимально. Для начала самый простой случай:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Бинарная классификация&lt;/li&gt;
  &lt;li&gt;Все аттрибуты categorical с двумя возможными значениями&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Иформация до разделения считается просто как энтропия:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; H(S) = - p_+ \log{p_+} - p_- \log{p_-} &lt;/script&gt;

&lt;p&gt;Здесь $p$ - пропорции положительных и отрицательных примеров соответственно во всей совокупности S&lt;/p&gt;

&lt;p&gt;Информация после разделения по аттрибуту $A$ складывается из двух частей (так как у каждого аттрибута только два значения $v_1$ и $v_2$) &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; H(S_1) = - p_+ \log{p_+} - p_- \log{p_-} &lt;/script&gt;

&lt;p&gt;Здесь $p$ - пропорции положительных и отрицательных в части примеров S, у которых $a = v_1$. Аналогично для $v_2$ имеем $H(S_2)$. &lt;/p&gt;

&lt;p&gt;Чтобы получить среднюю информацию после разделения, берём взвешенное среднее от $H(S_1)$ и $H(S_2)$. Получаем сколько информации взяло на себя разделение по этому атрибуту. Называется эта величина &lt;strong&gt;information gain&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Gain(S,A) = H(S) - p_1 * H(S_1) - p_2 * H(S_2) &lt;/script&gt;

&lt;p&gt;$p_1$ и $p_2$ - пропорции примеров, у которых $A = v_1$ и $A = v_2$ соответственно.&lt;/p&gt;

&lt;p&gt;Для ситуации, когда $S$ известна нам целиком (batch setting, не онлайн) все просто - выбираем лучший атрибут по значению gain’а, разделяем, выбираем следущий и т.д. 
Все интереснее в онлайне. &lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;Добавляем онлайн&lt;/h2&gt;

&lt;p&gt;Тут нам в помощь приходит следущее соотношение, называемое Hoeffding bound. Согласно ему, с вероятностью $1 - \delta$ истинное среднее случайной переменной с диапазоном значений $R$ не будет отличаться от оцененного среднего после $n$ независимых наблюдений больше, чем на &lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \epsilon = \sqrt{ \frac{R^2 \ln{(1/\delta)}}{2n} } &lt;/script&gt;

&lt;p&gt;В нашем случае за случайную величину берется разница в gain’е между лучшим и вторым лучшим атрибутом. Например, если границу $\epsilon$ мы оценили в 0.2, а посчитанная разница у нас 0.3, мы можем с увернностью сказать, что первый атрибут лучше второго как минимум на 0.1.&lt;/p&gt;

&lt;p&gt;В нашем случае, когда оценивается разница в gain’ах, за $R$ берется логарифм степени 2 от числа классов. $\delta$ - это точность оценки - параметр алгоритма, который можно изменять.&lt;/p&gt;

&lt;p&gt;Дальше все просто: копим статистику достаточную для того, чтобы делать значимые выводы о значениях gain’ов, затем выбираем 2 лучших атрибута и считаем Hoeffding bound между ними. &lt;/p&gt;

&lt;p&gt;Какие есть проблемы с таким подходом:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Дерево начинает сильно ветвиться без особого толку. Решение этой проблемы называется &lt;strong&gt;Pre-pruning&lt;/strong&gt;. Вводится дополнительный атрибут $A_0$, который значит “не разделять”. Тогда разделение призойдет, только если выбрали за лучший атрибут не $A_0$.&lt;/li&gt;
  &lt;li&gt;Два атрибута не сильно отличаются друг от друга в плане гейнов. Дерево будет очень долго ждать, чтобы хоть как-то разделиться. В критичном случае разделения вообще может не произойти. Решение &lt;strong&gt;Tie-breaking&lt;/strong&gt;. Если разница меньше определенного значения - брать лучший атрибут и не заморачиваться с тем, насколько близок к нему второй. &lt;/li&gt;
  &lt;li&gt;Атрибут дает разделение по gain’ам хорошее, но по факту большинство примеров попадают всегда в одну ветку. Решение &lt;strong&gt;Skewed Split prevention&lt;/strong&gt; - не разрешать такие сплиты.&lt;/li&gt;
  &lt;li&gt;Дерево постоянно растет и когда-нибудь упрется в память. Нужно уметь отключать разделение на “малообещающих” узлах. Те, до которых мы вряд ли дойдем, и которые и так не ошибаются. Делается регулярно, узлы включаются/отключаются. &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-2&quot;&gt;Числовые аттрибуты&lt;/h3&gt;

&lt;p&gt;Выше мы рассматрели только случай с категорийными атрибутами, разделение по ним делается в лоб. Как быть с числовыми? Можно использовать дискретизацию и перевести числовые атрибуты в категорийные:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Equal width&lt;/strong&gt;. Если известны границы, в которых атрибут меняется, он делится на k равных частей и преобразуется в значения от 0 до k в зависимости от части, в которую попадает его значение. В онлайне способ применим, если границы известны заранее.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Equal frequency&lt;/strong&gt;. Если известны границы, в которых атрибут меняется, он делится на k частей. Части не равные, выбираются таким образом, чтобы каждая содержала одинаковое число элементов. В таком простом виде в онлайне не применим (нужно заранее знать все значения).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;K-mean clustering&lt;/strong&gt;. Значения кластеризуются на k кластеров. Атрибут преобразуется в значения от 0 до k в зависимости от кластера, в который попадает. В таком простом виде в онлайне не применим.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Quantile Summaries&lt;/strong&gt;. Описан &lt;a href=&quot;http://infolab.stanford.edu/~datar/courses/cs361a/papers/quantiles.pdf&quot;&gt;тут&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gaussian Approximation&lt;/strong&gt;. Считаем, что для каждого класса значения атрибута будут представляться в виде нормального распределения (если у нас n классов, получим n распределений). Параметры распределения оцениваются по статистикам, которые накоплены в текущем узле. Дальше за разделение берется точка пересечений этих распределений.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-3&quot;&gt;Дальше&lt;/h3&gt;

&lt;p&gt;Что еще бывает в онлайне. Адаптивные деревья решений - используются, когда распределение входных данных меняется с течением времени. Ансамбли дервеьев на манер оффлайн тоже можно строить, но со своими ограничениями. Это вообще целая отдельная тема.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/decision-trees&quot;&gt;Онлайн деревья решений&lt;/a&gt; was originally published by Rodion at &lt;a href=&quot;&quot;&gt;Rodion's blog&lt;/a&gt; on June 21, 2014.&lt;/p&gt;</content>
</entry>


<entry>
  <title type="html"><![CDATA[DIY blog]]></title>
  <link rel="alternate" type="text/html" href="/diy" />
  <id>/diy</id>
  <published>2014-06-04T00:00:00+04:00</published>
  <updated>2014-06-04T00:00:00+04:00</updated>
  <author>
    <name>Rodion</name>
    <uri></uri>
    <email>rodion.zheludkov@gmail.com</email>
  </author>
  <content type="html">&lt;p&gt;The whole blog is static using &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; platform. You can check raw content here &lt;a href=&quot;https://github.com/rodion-zheludkov/blog&quot;&gt;https://github.com/rodion-zheludkov/blog&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://daringfireball.net/projects/markdown/&quot;&gt;Markdown&lt;/a&gt; formatting is used in raw source.&lt;/p&gt;

&lt;p&gt;Formulas are done with &lt;a href=&quot;http://www.mathjax.org/&quot;&gt;Mathjax&lt;/a&gt;.&lt;/p&gt;

  &lt;p&gt;&lt;a href=&quot;/diy&quot;&gt;DIY blog&lt;/a&gt; was originally published by Rodion at &lt;a href=&quot;&quot;&gt;Rodion's blog&lt;/a&gt; on June 04, 2014.&lt;/p&gt;</content>
</entry>

</feed>
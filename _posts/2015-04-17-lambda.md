---
layout: post
title:  "Lambda Architecture"
tags: [it, bigdata]
description: Обзорно про лямбда архитектуру.
published: false 
---

Лямбда архитектура это один из подходов к построению хранилища данных. Чтобы не рассуждать слишком абстрактно, все будем описывать на примере построения система для показа рекламных объявлений и предсказании CTR. Есть несколько ключевых особенностей. 

1. Храним все данные. Все показы объявлений, клики (ограничиваемся только объемами дисков). Как правило для большой рекламной сети, таких данных очень много, хранить их в базе не вариант. Обычно это какая-нибудь Кафка с выгрузкой в HFDS. 
2. Для того, чтобы уметь быстро вытаскивать интересующие данные из такого хранилища, надо поверх сырых данных строить materialized views. Как правило они пересчитываются полностью на какой-то переодической основе. В этом подходе стараются не использовать инкрементные представления, а пересчитывать каждый раз по всем доступным данным. Такой подход, как правило, сильно проще. Если код, который обновлял инкрементное представление, записал ошибочные данные - восстановление будет нетривиальным. Если мы пересчитываем все заново - просто перезапустить процесс с исправленным кодом. Этот переодический процесс называется **Batch Layer**. Реализуется, например, в виде MapReduce задач, с последующей выгрузкой в какое-нибудь быстрое хранилище. В нашем примере одним из таких view будет, например, кликовая статистика по дням в разрезе по рекламодателям.
3. Между переодическими запусками есть задержка (например в день, если batch layer запускается раз в сутки). Эти данные мы в наших view не видим. В лямбда подходе, для решения этой проблемы, делают еще один уровень. **Speed Layer**. Это реалтайм обработка данных из той же Кафки, которые еще не подхватились MR задачами. Записываются напрямую в какие-то быстрые инкрементые вьшки. Например на HBase.  
4. Нужен еще один уровень, который будет отвечать на пользовательский запросы, и отдавать объединенные результаты speed и batch процессов. 

![Lambda Arch](data/lambda.1.png)

*image from http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for*

![Lambda Arch](data/lambda.2.png)

*image from http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html*

Какие плюсы у такого подхода?

1. Данные неизменны. Каждое изменение хранится как новое событие с таймстемпом (событие клика, например). Это исключает повреждение данных из-за некорректных апдейтов.
2. Всегда можно пересчитать с нуля все результаты. Например, надо добавить новое поле во view, просто добавляем код, которые его вычисляет и перезапускаем batch и speed уровни.

Минусы

1. У нас получится два совсем разных кода и две инфраструктуры. Например, одну и туже логику придется написать в MR и в Storm'е.
2. Serving Layer обычно получается сложным.

# Kappa Architecture

Ребята из линкедина, написали статью как избавится от этих минусов. Перенеся всю обработку в реалтайм. Назвали подход Kappa Architecture. 

1. Использовать кафку или любую другую систему, в которой можно хранить все входные события и есть подписка на поток данных.
2. Когда надо пересчитать вьюшки - стартуем второй экземпляр нашей системы и читаем все данные с самого начала. Выход тоже перенаправляем в новые таблицы. 
3. Когда вторая задача догнала и закончила репроцессинг, переключаем читателей на новые таблицы.
4. Отключаем 1ую задачу, вторая продолжает работать.

Как в таком подходе работать с операциям, которым нужен state? Например группировка и агрегация сообщений. На MR все понятно. Как их перенести на логику обработки потоков? 

Прежде всего надо разобраться с понятием группировки сообщений. Во всех фреймворках, типа сторма, вы можете разбивать сообщения по какому-нибудь id. Таким образом, что все сообщения с одним id попадут на один обработчик. Это и есть группировка. Самый простой способ группировки - по хешу task = hash(ID) % NUM_TASKS.

Рассмотрим пару примеров задач, которым нужно состояние.

**Посчитать CTR'ы для каждого объявления**

Нужны счетчики - показы и клики. Это пример *windowed aggregation*. Используем группинг по bannerid, в простейшему случае счетчики можно держать в памяти и писать в базу, по окончанию каждого временного окна (например, сбрасывать раз в час).

*TODO* add pic

**Посчитать CTR'ы для типа устройства (desktop/mobile)**

В этой задаче надо агрегировать данные по типу устройства. Тип устройства можем получить из user agent'а. Это пример джоина стрим данных (клики, показы) с какой-то таблицей (информация о пользователе). Первый вариант реализации - на каждое событие ходим во внешнее хранилище пользовательской информации. Но скорее всего будет медленно. Лучше хранить информацию по части пользователей локально относительно таска. Надо убедиться, что таск будет получать евенты, только для этих пользователей. Опять на помощь приходит группинг hash(userId) % NUM_TASKS. Как залить данные локально? Можно все изменения пользовательской информации тоже представлять как события и записывать в кафку, тогда они будут прилетать в этот же таски и мы сможем записывать их локально. 
Дальше просто агрегируем каунты и регулярно их сбрасываем в базу. Что если машинка с таском зафейлилась? Можем перезапустить таск на другой машине предварительно считав весь user changelog из кафки, чтобы восстановить последний state пользователя. 

*TODO* add pic

# Ссылки

*http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html*

*http://samza.apache.org/learn/documentation/0.7.0/container/state-management.html*
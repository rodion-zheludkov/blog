<!doctype html>
<html>
<head>
<meta charset="utf-8">
<title>Онлайн деревья решений &#8211; Rodion's blog</title>
<meta name="description" content="Деревья решений, information gain и как это применить в реальном времени">
<meta name="keywords" content="it, ml">

<link rel="stylesheet" href="/assets/css/style.css">
<link href='http://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>


</head>
<body>

<div class="container-fluid main">
    <div class="main-wrap">
        <div class="main-left">
            <h1>Онлайн деревья решений</h1>
<p>Задачи машинного обучения обычно сводятся к тому, чтобы посторить модель предсказания реальности и минимизировать ее ошибку. В зависимости от выбора модели и функции ошибки мы получим разные алгоритмы. Но можно рассматривать эту задачу немного с другой стороны: ошибки - это разница между нашей моделью и реальным миром. Чем больше модель ошибается, тем больше неожиданных предсказаний мы получим и, сравнивая предсказания с реальностью, мы сильно <em>удивимся</em>. </p>

<p>То есть в каком-то смысле задача состоит в том, чтобы уменьшить наше удивление и сделать модель, соответсвующую реальности максимально. Собственно, первый же вопрос, который возникает “а как это удивление мерить?”. Этим занимается отдельная наука - теория информации. Она рассматривает информацию, связанную с исходом случайной величины и отвечает на вопросы вида “сколько информации несет нам знание того, что на кубике выпала “6”. Интуитивно информация соответствует мере удивления при появлении какого-то исхода. Чем больше мы удивлемся, тем больше информации получаем. Выбираются следущие аксиомы:</p>

<ol>
  <li>$I(p)$ непрерывная монотонно уменьшающаяся функция от $p$. $p$ - вероятность рассматриваемого исхода</li>
  <li>Если A и B независимые события. $P(A) = p_1, P(B) = p_2, P(AB)=p_1p_2$. Тогда $I(p_1p_2) = I(p_1) + I(p_2)$. </li>
</ol>

<p>1-ая аксима по сути означает, что менее вероятному событию мы удивляемся больше. Например, выпадению 6x6 на двух кубиках мы удивимся больше, чем выпадению орла при бросании монеты. </p>

<p>2-ая аксиома говорит, что удивления от невлияющих друг на друга событий складывается из удивлений от каждого отдельного события. Тоже все логично и не противоречит интуитивному пониманию.</p>

<p>Можно показать, что единственное семейство функций, которое удовлетворяет этим аксиомам, это </p>

<script type="math/tex; mode=display"> I = -\log{p_i} </script>

<p>Теперь нам нужна некая единица измерения информации. Возьмем за 1 бит неопределенность, связанную с исходом случайной величины, которая может быть в двух состояниях с вероятностью = 1/2. По сути это удивления от исхода после броска монеты. Тогда база логарифма будет 2, а мера изменения будет называться “бит”. Важно, что это не тот же бит что в компьютерах (принимающий только значения 0 и 1). Информация может измеряться и в дробных значениях. Это просто мера удивления, например, 1.5 бита значит, что мы в полтора раза больше удивляемся данному исходу по сравнению с результатом броска монеты.</p>

<p>Вообще говоря, нас интересует не один конкретный исход, а все множество исходов, образующих случайную величину. Мы хотим уметь сравнивать разные случайные величины по тому, насколько одна содержит больше информации (“удивительнее”) другой. Тут все просто: берем мат. ожидание удивления от всех событий</p>

<script type="math/tex; mode=display"> H = -\sum{p_i}*\log{p_i} </script>

<ul>
  <li>$I$ - это мера удивления от одного исхода; </li>
  <li>$H$ - мера удивления всей случайной величины. Она называется <strong>entropy</strong>.</li>
</ul>

<h2 id="section">Переходим непосредственно к деревьям решений</h2>

<p>Теперь нам надо построить дерево решений. Первым шагом надо выбрать узел, который максимально отделит одни классы от других. Если рассматривать эту задачу с точки зрения теории информации, нам надо выбрать узел таким образом, чтобы наше удивление уменьшилось максимально. Для начала самый простой случай:</p>

<ul>
  <li>Бинарная классификация</li>
  <li>Все аттрибуты categorical с двумя возможными значениями</li>
</ul>

<p>Иформация до разделения считается просто как энтропия:</p>

<script type="math/tex; mode=display"> H(S) = - p_+ \log{p_+} - p_- \log{p_-} </script>

<p>Здесь $p$ - пропорции положительных и отрицательных примеров соответственно во всей совокупности S</p>

<p>Информация после разделения по аттрибуту $A$ складывается из двух частей (так как у каждого аттрибута только два значения $v_1$ и $v_2$) </p>

<script type="math/tex; mode=display"> H(S_1) = - p_+ \log{p_+} - p_- \log{p_-} </script>

<p>Здесь $p$ - пропорции положительных и отрицательных в части примеров S, у которых $a = v_1$. Аналогично для $v_2$ имеем $H(S_2)$. </p>

<p>Чтобы получить среднюю информацию после разделения, берём взвешенное среднее от $H(S_1)$ и $H(S_2)$. Получаем сколько информации взяло на себя разделение по этому атрибуту. Называется эта величина <strong>information gain</strong></p>

<script type="math/tex; mode=display"> Gain(S,A) = H(S) - p_1 * H(S_1) - p_2 * H(S_2) </script>

<p>$p_1$ и $p_2$ - пропорции примеров, у которых $A = v_1$ и $A = v_2$ соответственно.</p>

<p>Для ситуации, когда $S$ известна нам целиком (batch setting, не онлайн) все просто - выбираем лучший атрибут по значению gain’а, разделяем, выбираем следущий и т.д. 
Все интереснее в онлайне. </p>

<h2 id="section-1">Добавляем онлайн</h2>

<p>Тут нам в помощь приходит следущее соотношение, называемое Hoeffding bound. Согласно ему, с вероятностью $1 - \delta$ истинное среднее случайной переменной с диапазоном значений $R$ не будет отличаться от оцененного среднего после $n$ независимых наблюдений больше, чем на </p>

<script type="math/tex; mode=display"> \epsilon = \sqrt{ \frac{R^2 \ln{(1/\delta)}}{2n} } </script>

<p>В нашем случае за случайную величину берется разница в gain’е между лучшим и вторым лучшим атрибутом. Например, если границу $\epsilon$ мы оценили в 0.2, а посчитанная разница у нас 0.3, мы можем с увернностью сказать, что первый атрибут лучше второго как минимум на 0.1.</p>

<p>В нашем случае, когда оценивается разница в gain’ах, за $R$ берется логарифм степени 2 от числа классов. $\delta$ - это точность оценки - параметр алгоритма, который можно изменять.</p>

<p>Дальше все просто: копим статистику достаточную для того, чтобы делать значимые выводы о значениях gain’ов, затем выбираем 2 лучших атрибута и считаем Hoeffding bound между ними. </p>

<p>Какие есть проблемы с таким подходом:</p>

<ol>
  <li>Дерево начинает сильно ветвиться без особого толку. Решение этой проблемы называется <strong>Pre-pruning</strong>. Вводится дополнительный атрибут $A_0$, который значит “не разделять”. Тогда разделение призойдет, только если выбрали за лучший атрибут не $A_0$.</li>
  <li>Два атрибута не сильно отличаются друг от друга в плане гейнов. Дерево будет очень долго ждать, чтобы хоть как-то разделиться. В критичном случае разделения вообще может не произойти. Решение <strong>Tie-breaking</strong>. Если разница меньше определенного значения - брать лучший атрибут и не заморачиваться с тем, насколько близок к нему второй. </li>
  <li>Атрибут дает разделение по gain’ам хорошее, но по факту большинство примеров попадают всегда в одну ветку. Решение <strong>Skewed Split prevention</strong> - не разрешать такие сплиты.</li>
  <li>Дерево постоянно растет и когда-нибудь упрется в память. Нужно уметь отключать разделение на “малообещающих” узлах. Те, до которых мы вряд ли дойдем, и которые и так не ошибаются. Делается регулярно, узлы включаются/отключаются. </li>
</ol>

<h3 id="section-2">Числовые аттрибуты</h3>

<p>Выше мы рассматрели только случай с категорийными атрибутами, разделение по ним делается в лоб. Как быть с числовыми? Можно использовать дискретизацию и перевести числовые атрибуты в категорийные:</p>

<ol>
  <li><strong>Equal width</strong>. Если известны границы, в которых атрибут меняется, он делится на k равных частей и преобразуется в значения от 0 до k в зависимости от части, в которую попадает его значение. В онлайне способ применим, если границы известны заранее.</li>
  <li><strong>Equal frequency</strong>. Если известны границы, в которых атрибут меняется, он делится на k частей. Части не равные, выбираются таким образом, чтобы каждая содержала одинаковое число элементов. В таком простом виде в онлайне не применим (нужно заранее знать все значения).</li>
  <li><strong>K-mean clustering</strong>. Значения кластеризуются на k кластеров. Атрибут преобразуется в значения от 0 до k в зависимости от кластера, в который попадает. В таком простом виде в онлайне не применим.</li>
  <li><strong>Quantile Summaries</strong>. Описан <a href="http://infolab.stanford.edu/~datar/courses/cs361a/papers/quantiles.pdf">тут</a></li>
  <li><strong>Gaussian Approximation</strong>. Считаем, что для каждого класса значения атрибута будут представляться в виде нормального распределения (если у нас n классов, получим n распределений). Параметры распределения оцениваются по статистикам, которые накоплены в текущем узле. Дальше за разделение берется точка пересечений этих распределений.</li>
</ol>

<h3 id="section-3">Дальше</h3>

<p>Что еще бывает в онлайне. Адаптивные деревья решений - используются, когда распределение входных данных меняется с течением времени. Ансамбли дервеьев на манер оффлайн тоже можно строить, но со своими ограничениями. Это вообще целая отдельная тема.</p>


        </div>
        <div class="main-right">
            <h3><a href="/">Rodion's blog</a></h3>
<small>put it on the cloud and kill it with fire</small>
<div>
    <b>Tags</b>
    <ul>
        
        <li><a href="/tags/it">it</a></li>
        
        <li><a href="/tags/ml">ml</a></li>
        
    </ul>
</div>

<div>
    <b>Connect</b>
    <ul>
        
        <li><a href="http://linkedin.com/in/rzheludkov" target="_blank"><i class="icon-linkedin"></i> LinkedIn</a></li>
        
        
        <li><a href="http://facebook.com/rodion.zheludkov" target="_blank"><i class="icon-facebook"></i> Facebook</a></li>
        
    </ul>
</div>


        </div>
    </div>
</div>

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$']]
    },
    showMathMenu: false,
    showMathMenuMSIE: false
});
</script>         

</body>
</html>